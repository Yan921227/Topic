"""
ç°¡æ˜“ç‰ˆå½±ç‰‡è¾¨è­˜ç¨‹å¼
ç›´æ¥ä¿®æ”¹ä¸‹é¢çš„è¨­å®šå³å¯ä½¿ç”¨
"""
from ultralytics import YOLO
import cv2
from pathlib import Path
import torch

# ============= è¨­å®šå€ =============
VIDEO_PATH = "C:\\Users\\User\\Desktop\\IMG_1159.mp4"              # è¼¸å…¥å½±ç‰‡è·¯å¾‘
MODEL_PATH = "runs\\detect\\train3\\weights\\best.pt"  # æ¨¡å‹è·¯å¾‘
OUTPUT_DIR = "output_frames\\20251106m_model"                # è¼¸å‡ºç›®éŒ„

# çµ±ä¸€ä¿¡å¿ƒåº¦ï¼ˆç”¨æ–¼åˆæ­¥éæ¿¾ï¼Œå»ºè­°è¨­ä½ä¸€é»ï¼‰
BASE_CONFIDENCE = 0.25

# é‡å°ä¸åŒé¡åˆ¥è¨­å®šå„è‡ªçš„ä¿¡å¿ƒåº¦é–¾å€¼
CLASS_CONFIDENCE = {
    0: 0.85,  # backswingï¼ˆå¾Œæ“ºå‹•ä½œï¼‰
    1: 0.62,  # impactï¼ˆæ“Šçƒç¬é–“ï¼‰
}

SAVE_INTERVAL = 1                           # æ¯éš”å¹¾å¹€å­˜ä¸€å¼µ (1=æ¯å¹€éƒ½å­˜)
# ==================================

if __name__ == "__main__":
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è¼‰å…¥æ¨¡å‹
    print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {MODEL_PATH}")
    model = YOLO(MODEL_PATH)
    
    # é–‹å•Ÿå½±ç‰‡
    print(f"ğŸ”„ é–‹å•Ÿå½±ç‰‡: {VIDEO_PATH}")
    cap = cv2.VideoCapture(VIDEO_PATH)
    
    if not cap.isOpened():
        print("âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡æª”æ¡ˆï¼è«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢º")
        exit()
    
    # å–å¾—å½±ç‰‡è³‡è¨Š
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"\nğŸ“Š å½±ç‰‡è³‡è¨Š:")
    print(f"   ç¸½å¹€æ•¸: {total_frames}")
    print(f"   FPS: {fps}")
    print(f"   è§£æåº¦: {width}x{height}")
    print(f"   é è¨ˆè¼¸å‡º: {total_frames // SAVE_INTERVAL} å¼µåœ–ç‰‡")
    print(f"\nğŸš€ é–‹å§‹è¾¨è­˜...\n")
    
    frame_count = 0
    saved_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # æ ¹æ“šé–“éš”æ±ºå®šæ˜¯å¦è™•ç†
        if frame_count % SAVE_INTERVAL != 0:
            continue
        
        # é€²è¡Œè¾¨è­˜ï¼ˆä½¿ç”¨è¼ƒä½çš„åŸºç¤ä¿¡å¿ƒåº¦ï¼‰
        results = model(frame, conf=BASE_CONFIDENCE, verbose=False)
        
        # æ ¹æ“šå„é¡åˆ¥çš„ä¿¡å¿ƒåº¦é–¾å€¼éæ¿¾çµæœ
        boxes = results[0].boxes
        
        if boxes is not None and len(boxes) > 0:
            filtered_indices = []
            
            for i, box in enumerate(boxes):
                class_id = int(box.cls[0])  # é¡åˆ¥ID
                confidence = float(box.conf[0])  # ä¿¡å¿ƒåº¦
                
                # æª¢æŸ¥é€™å€‹é¡åˆ¥æ˜¯å¦æœ‰è¨­å®šå°ˆå±¬é–¾å€¼
                if class_id in CLASS_CONFIDENCE:
                    threshold = CLASS_CONFIDENCE[class_id]
                else:
                    threshold = BASE_CONFIDENCE  # æ²’è¨­å®šå°±ç”¨åŸºç¤é–¾å€¼
                
                # åªä¿ç•™ç¬¦åˆè©²é¡åˆ¥é–¾å€¼çš„åµæ¸¬çµæœ
                if confidence >= threshold:
                    filtered_indices.append(i)
            
            # å¦‚æœæœ‰ç¬¦åˆæ¢ä»¶çš„æ¡†ï¼Œä¿ç•™å®ƒå€‘ï¼›å¦å‰‡æ¸…ç©º
            if filtered_indices:
                # ä¿ç•™ç¬¦åˆæ¢ä»¶çš„ boxes
                import torch
                boxes.data = boxes.data[filtered_indices]
            else:
                # æ¸…ç©ºæ‰€æœ‰ boxes
                boxes.data = boxes.data[:0]
        
        # ç¹ªè£½çµæœ
        annotated_frame = results[0].plot()
        
        # å„²å­˜åœ–ç‰‡
        output_filename = output_path / f"frame_{frame_count:06d}.jpg"
        cv2.imwrite(str(output_filename), annotated_frame)
        saved_count += 1
        
        # é¡¯ç¤ºé€²åº¦
        if saved_count % 10 == 0:
            progress = (frame_count / total_frames) * 100
            detections = len(results[0].boxes)
            print(f"é€²åº¦: {progress:.1f}% | å·²å„²å­˜: {saved_count} å¼µ | åµæ¸¬åˆ°: {detections} å€‹ç‰©ä»¶")
    
    cap.release()
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"ğŸ“ å·²å„²å­˜ {saved_count} å¼µåœ–ç‰‡åˆ°: {output_path.absolute()}")

